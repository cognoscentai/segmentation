@techreport{segmentation-tr,
            type = {Technical Report},
           title = {Quality Evaluation Methods for Crowdsourced Image Segmentation},
          author = {Doris Jung-Lin Lee and Akash Das Sarma and Aditya Parameswaran},
       publisher = {Stanford InfoLab},
     institution = {Stanford University},
             url = {http://ilpubs.stanford.edu:8090/1161/},
             year= {2018},
        abstract = {Instance-level image segmentation provides rich information crucial for scene understanding in a variety of real-world applications. In this paper, we evaluate multiple crowdsourced algorithms for the image segmentation problem, including novel worker-aggregation-based methods and retrieval-based methods from prior work. We charac- terize the different types of worker errors observed in crowdsourced segmentation, and present a clustering algorithm as a preprocess- ing step that is able to capture and eliminate errors arising due to workers having different semantic perspectives. We demonstrate that aggregation-based algorithms attain higher accuracies than exist- ing retrieval-based approaches, while scaling better with increasing numbers of worker segmentations.}
}


@article{felzenszwalb2004efficient,
  title={Efficient graph-based image segmentation},
  author={Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
  journal={International journal of computer vision},
  volume={59},
  number={2},
  pages={167--181},
  year={2004},
  publisher={Springer}
}


@article{Irshad2014,
author = {Irshad, H and Montaser-Kouhsari et. al.},
doi = {10.1142/9789814644730_0029},
file = {:Users/dorislee/Dropbox/Papers/IRSHAD et al.{\_}2014{\_}Crowdsourcing Image Annotation for Nucleus Detection and Segmentation in Computational Pathology Evaluating Experts,.pdf:pdf},
isbn = {978-981-4644-72-3},
journal = {Biocomputing 2015},
pages = {294--305},
title = {{Crowdsourcing Image Annotation for Nucleus Detection and Segmentation in Computational Pathology: Evaluating Experts, Automated Methods, and the Crowd}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9789814644730{\_}0029},
year = {2014}
}
@article{Bearman2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02106v5},
author = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and Fei-fei, Li},
doi = {10.1007/978-3-319-46478-7},
eprint = {arXiv:1506.02106v5},
file = {:Users/dorislee/Dropbox/Papers/Bearman et al.{\_}2016{\_}What's the Point Semantic Segmentation with Point Supervision.pdf:pdf},
isbn = {9783319464787},
journal = {Eccv'16},
keywords = {data annotation,semantic segmentation,weak supervision},
mendeley-groups = {HCI/AI+HCI/crowd/crowd-seg},
pages = {1--16},
title = {{What's the Point : Semantic Segmentation with Point Supervision}},
year = {2016}
}

@article{Y.Y.Boykov2001,
author = {Y.Y.Boykov and M-P.Jolly},
file = {:Users/dorislee/Dropbox/Papers/Boykov{\_}2001{\_}Interactive Graph Cuts for Optimal Boundary {\&} Region Segmentation of Objects in textup{\{}N{\}}-textup{\{}D{\}} Images.pdf:pdf},
isbn = {0769511430},
journal = {Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on},
number = {July},
pages = {105--112},
title = {{Interactive Graph Cuts for Optimal Boundary {\&} Region Segmentation of Objects in $\backslash$textup{\{}N{\}}-$\backslash$textup{\{}D{\}} Images}},
year = {2001}
}
@article{Li2009,
abstract = {Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a dasiapolo gamepsila consists of several visual objects such as dasiahumanpsila, dasiahorsepsila, dasiagrasspsila, etc. In addition, it can be further annotated with a list of more abstract (e.g. dasiaduskpsila) or visually less salient (e.g. dasiasaddlepsila) tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms state-of-the-art algorithms.},
author = {Li, Li Jia and Socher, Richard and Fei-Fei, Li},
doi = {10.1109/CVPRW.2009.5206718},
file = {:Users/dorislee/Dropbox/Papers/Li, Socher{\_}Unknown{\_}Towards Total Scene Understanding Classification , Annotation and Segmentation in an Automatic Framework Scene polo.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {2036--2043},
title = {{Towards total scene understanding: Classification, annotation and segmentation in an automatic framework}},
year = {2009}
}
@article{Kokkinos2008,
author = {Kokkinos, Iasonas and Maragos, Petros},
file = {:Users/dorislee/Dropbox/Papers/Kokkinos, Maragos{\_}2008{\_}Synergy Between Object Recognition and Image Segmentation using the Expectation Maximization Algorithm.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
mendeley-groups = {seg},
number = {20},
pages = {1--16},
title = {{Synergy Between Object Recognition and Image Segmentation using the Expectation Maximization Algorithm}},
url = {http://www.mas.ecp.fr/vision/Personnel/iasonas/pubs/KokkinosMaragos{\_}EM{\_}PAMI09.pdf{\%}5Cnpapers2://publication/uuid/E47A86BD-26D3-464A-8F3B-A556D2462657},
volume = {20},
year = {2008}
}
@article{Lin2012,
abstract = {To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LazySusan, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LazySusan at solving SAT Math questions, eliminating 83.2{\%} of the error and achieving greater net utility compared to the state-ofthe- art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.},
archivePrefix = {arXiv},
arxivId = {arXiv preprint arXiv:1210.4870.},
author = {Lin, Christopher H and Mausam and Weld, Daniel S},
eprint = {arXiv preprint arXiv:1210.4870.},
file = {:Users/dorislee/Dropbox/Papers/Lin, Mausam, Weld{\_}2012{\_}Crowdsourcing control Moving beyond multiple choice.pdf:pdf},
isbn = {9780974903989},
journal = {AAAI Conference on Human Computation and Crowdsourcing (HCOMP)},
mendeley-groups = {seg},
pages = {491--500},
title = {{Crowdsourcing control : Moving beyond multiple choice}},
year = {2012}
}
@article{Karger2013,
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an e ective large-scale human-powered platform for performing tasks in domains such as image classi cation, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-o between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1  " as long as the redundancy per task is O  (K=q) log(K=")  , where each task can have any of the K distinct answers equally likely, q is the crowd-quality parameter that is de ned through a probabilistic model. Further, e ectively this is the best possible redundancy-accuracy trade-o any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-o between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their in uence on the redundancy-accuracy trade-o . Unlike recent prior work [13, 17, 19], our result applies to non-binary (i.e. K {\textgreater} 2) tasks. In e ect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [13, 17, 19]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
author = {Karger, David R and Oh, Sewoong and Shah, Devavrat},
doi = {10.1145/2494232.2465761},
file = {:Users/dorislee/Dropbox/Papers/Karger, Oh, Shah{\_}2013{\_}Efficient crowdsourcing for multi-class labeling.pdf:pdf},
isbn = {978-1-4503-1900-3},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {crowdsourcing,human computation,low-rank matrix,random graphs},
mendeley-groups = {seg},
number = {1},
pages = {81},
title = {{Efficient crowdsourcing for multi-class labeling}},
url = {http://dl.acm.org/citation.cfm?id=2494232.2465761},
volume = {41},
year = {2013}
}
@article{Liu2012,
abstract = {Crowdsourcing has become a popular paradigm for labeling large datasets. How- ever, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this prob- lem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both major- ity voting and a recent algorithm by Karger et al. [1], while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribu- tion on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.},
author = {Liu, Qiang and Peng, Jian and Ihler, Alex},
file = {:Users/dorislee/Dropbox/Papers/Liu, Peng, Ihler{\_}2012{\_}Variational Inference for Crowdsourcing.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
mendeley-groups = {seg},
pages = {701--709},
title = {{Variational Inference for Crowdsourcing}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0328.pdf{\%}5Cnpapers2://publication/uuid/5BE089DC-9B5E-4494-85ED-5926E1560F82},
year = {2012}
}
@article{Heim2016,
archivePrefix = {arXiv},
arxivId = {1611.08527},
author = {Eccv, Anonymous},
eprint = {1611.08527},
file = {:Users/dorislee/Dropbox/Papers/Eccv{\_}Unknown{\_}Clickstream analysis for crowd-based object segmentation with confidence.pdf:pdf},
author={Eric Heim, Alexander Seitel, Christian Stock, Fabian Isensee, Lena Maier-Hein},
keywords = {clickstream analysis,con-,crowdsourcing,fidence estimation,object segmentation,quality control},
mendeley-groups = {seg},
year={2016},
pages = {1--13},
title = {{Clickstream analysis for crowd-based object segmentation with confidence}}
}
@article{Liu2011,
abstract = {In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.},
author = {Liu, Tie and Yuan, Zejian and Sun, Jian and Wang, Jingdong and Zheng, Nanning and Tang, Xiaoou and Shum, Heung-Yeung},
doi = {10.1109/TPAMI.2010.70},
file = {:Users/dorislee/Dropbox/Papers/Liu et al.{\_}2011{\_}Learning to detect a salient object.pdf:pdf},
isbn = {1424411807},
issn = {1939-3539},
journal = {Computer Vision and Pattern Recognition (CVPR)},
mendeley-groups = {seg},
number = {2},
pages = {1--8},
pmid = {21193811},
title = {{Learning to detect a salient object}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5432215{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/21193811},
volume = {33},
year = {2011}
}
@article{bell15minc,
	author = "Sean Bell and Paul Upchurch and Noah Snavely and Kavita Bala",
	title = "Material Recognition in the Wild with the Materials in Context Database",
	journal = "Computer Vision and Pattern Recognition (CVPR)",
	year = "2015",
}
@article{bell14intrinsic,
	author = "Sean Bell and Kavita Bala and Noah Snavely",
	title = "Intrinsic Images in the Wild",
	journal = "ACM Trans. on Graphics (SIGGRAPH)",
	volume = "33",
	number = "4",
	year = "2014",
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {European Conference on Computer Vision (ECCV)},
mendeley-groups = {seg},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Torralba2010,
abstract = {Central to the development of computer vision systems is the collection and use of annotated images spanning our visual world. Annotations may include information about the identity, spatial extent, and viewpoint of the objects present in a depicted scene. Such a database is useful for the training and evaluation of computer vision systems. Motivated by the availability of images on the Internet, we introduced a web-based annotation tool that allows online users to label objects and their spatial extent in images. To date, we have collected over 400 000 annotations that span a variety of different scene and object classes. In this paper, we show the contents of the database, its growth over time, and statistics of its usage. In addition, we explore and survey applications of the database in the areas of computer vision and computer graphics. Particularly, we show how to extract the real-world 3-D coordinates of images in a variety of scenes using only the user-provided object annotations. The output 3-D information is comparable to the quality produced by a laser range scanner. We also characterize the space of the images in the database by analyzing 1) statistics of the co-occurrence of large objects in the images and 2) the spatial layout of the labeled images.},
author = {Torralba, Antonio and Russell, Bryan C. and Yuen, Jenny},
doi = {10.1109/JPROC.2010.2050290},
file = {:Users/dorislee/Dropbox/Papers/Torralba, Russell, Yuen{\_}2010{\_}LabelMe Online image annotation and applications.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {3-D,Image database,Image statistics,Object detection,Object recognition,Online annotation tool,Video annotation},
mendeley-groups = {seg},
number = {8},
pages = {1467--1484},
title = {{LabelMe: Online image annotation and applications}},
volume = {98},
year = {2010}
}
@article{Cabezas2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00145v1},
author = {Cabezas, Ferran and Carlier, Axel and Charvillat, Vincent and Salvador, Amaia and Giro-I-Nieto, Xavier},
doi = {10.1109/ICIP.2015.7351606},
eprint = {arXiv:1505.00145v1},
file = {:Users/dorislee/Dropbox/Papers/Cabezas et al.{\_}2015{\_}Quality control in crowdsourced object segmentation.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
journal = {Proceedings of International Conference on Image Processing, ICIP},
keywords = {Crowdsourcing,Interactive Segmentation,Object Segmentation,Quality Control,Superpixel},
mendeley-groups = {seg},
pages = {4243--4247},
title = {{Quality control in crowdsourced object segmentation}},
volume = {2015-Decem},
year = {2015}
}
@article{AdrianaKovashka2016,
abstract = {The Path to Path-Traced Movies},
author = {{Adriana Kovashka} and {Olga Russakovsky} and {Li Fei-Fei}},
doi = {10.1561/0600000073},
file = {:Users/dorislee/Dropbox/Papers/Adriana Kovashka, Olga Russakovsky, Li Fei-Fei{\_}2016{\_}Crowdsourcing in Computer Vision.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
keywords = {Computer Graphics,Rendering,Rendering: Forward rendering},
mendeley-groups = {seg},
number = {2},
pages = {103--175},
title = {{Crowdsourcing in Computer Vision}},
url = {http://www.nowpublishers.com/article/Details/CGV-073},
volume = {10},
year = {2016}
}
@article{Raykar2009,
abstract = {We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.},
author = {Raykar, Vikas C},
doi = {10.1145/1553374.1553488},
file = {:Users/dorislee/Dropbox/Papers/Raykar{\_}2009{\_}Supervised Learning from Multiple Experts Whom to trust when everyone lies a bit.pdf:pdf},
isbn = {9781605585161},
journal = {New York},
mendeley-groups = {seg},
pages = {1--8},
title = {{Supervised Learning from Multiple Experts : Whom to trust when everyone lies a bit}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553488},
year = {2009}
}
@article{A.KovashkaO.Russakovsky2016,
abstract = {The Path to Path-Traced Movies},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.02145v1},
author = {{A. Kovashka, O. Russakovsky}, L. Fei-Fei and Grauman, K.},
doi = {10.1561/0600000073},
eprint = {arXiv:1611.02145v1},
file = {:Users/dorislee/Dropbox/Papers/A. Kovashka, O. Russakovsky, Grauman{\_}2016{\_}Crowdsourcing in Computer Vision.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
keywords = {Computer Graphics,Rendering,Rendering: Forward rendering},
mendeley-groups = {HCI/AI+HCI/crowd/crowd-seg},
number = {2},
pages = {103--175},
title = {{Crowdsourcing in Computer Vision}},
url = {http://www.nowpublishers.com/article/Details/CGV-073},
volume = {10},
year = {2016}
}
@inproceedings{Yamaguchi2012,
 author = {Yamaguchi, Kota},
 title = {Parsing Clothing in Fashion Photographs},
 booktitle = {Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 series = {CVPR '12},
 year = {2012},
 isbn = {978-1-4673-1226-4},
 pages = {3570--3577},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2354409.2355126},
 acmid = {2355126},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Clothing,Estimation,Labeling,Training,Visualization,Prototypes,Joints},
} 

@article{Whitehill2009,
abstract = {Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector 9). New Internet-based services allow for a large number of labelers to collab- orate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to si- multaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used Majority Vote heuristic for inferring image labels, and is robust to both noisy and adversarial labelers.},
author = {Whitehill, Jacob and Ruvolo, Paul and Wu, Tingfan and Bergsma, Jacob and Movellan, Javier},
file = {:Users/dorislee/Dropbox/Papers/Whitehill et al.{\_}2009{\_}Whose Vote Should Count More Optimal Integration of Labels from Labelers of Unknown Expertise.pdf:pdf},
isbn = {9781615679119},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {seg},
number = {1},
pages = {1--9},
title = {{Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise}},
volume = {22},
year = {2009}
}
@article{Dawid1979,
author = {Dawid, A. P. and Skene, A. M.},
file = {:Users/dorislee/Dropbox/Papers/Dawid, Skene{\_}1979{\_}Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.pdf:pdf},
keywords = {em algorithm,latent class model,medical example,observer variation},
mendeley-groups = {seg},
number = {1},
pages = {20--28},
title = {{Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm}},
volume = {28},
year = {1979}
}
@article{Ipeirotis2010,
abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsu-pervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings. {\textcopyright} 2010 ACM.},
author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
doi = {10.1145/1837885.1837906},
file = {:Users/dorislee/Dropbox/Papers/Ipeirotis, Provost, Wang{\_}2010{\_}Quality management on Amazon Mechanical Turk.pdf:pdf},
isbn = {9781450302227},
issn = {145030222X},
journal = {Proceedings of the ACM SIGKDD Workshop on Human Computation - HCOMP '10},
mendeley-groups = {seg},
pages = {64},
pmid = {23835650},
title = {{Quality management on Amazon Mechanical Turk}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77956245055{\&}partnerID=tZOtx3y1},
year = {2010}
}
@article{MDWWelinder2010,
abstract = {Distributing labeling tasks among hundreds or thousands of annotators is an in- creasingly important method for annotating large datasets. We present a method for estimating the underlying value (eg the class) of each image from (noisy) an- notations provided by multiple annotators. ...},
author = {Welinder, Peter and Branson, Steve and Belongie, Serge and Perona, Pietro},
doi = {10.1.1.231.1538},
file = {:Users/dorislee/Dropbox/Papers/Welinder et al.{\_}2010{\_}The Multidimensional Wisdom of Crowds.pdf:pdf},
isbn = {9781617823800},
journal = {NIPS (Conference on Neural Information Processing Systems)},
mendeley-groups = {seg},
pages = {1--9},
title = {{The Multidimensional Wisdom of Crowds}},
url = {http://www.vision.caltech.edu/visipedia/papers/WelinderEtalNIPS10.pdf},
volume = {6},
year = {2010}
}
@article{Russakovsky2015,
author = {Russakovsky, Olga and Li, Li-Jia and Fei-Fei, Li},
file = {:Users/dorislee/Dropbox/Papers/Russakovsky, Li, Fei-Fei{\_}2015{\_}Best of Both Worlds Human-Machine Collaboration for Object Annotation.pdf:pdf},
isbn = {9781467369640},
mendeley-groups = {seg},
pages = {2121--2131},
title = {{Best of Both Worlds: Human-Machine Collaboration for Object Annotation}},
year = {2015}
}
@article{Vittayakorn2011,
abstract = {As computer vision datasets grow larger, the community is increasingly relying on crowd-sourced annotations to train and test their algorithms. Since the capability of online annotators is considered varied and unpredictable, many strategies have been proposed to " clean " crowd-sourced annotations. However, these strategies typically require more annotations, rather than using the annotation or image content itself. In this paper we propose and evaluate several strategies for automatically estimating the quality of an object annotation. Finally, we show that we can significantly outperform simple baselines by combining multiple image-based anno-tation assessment strategies.},
author = {Vittayakorn, Sirion and Hays, James},
doi = {10.5244/C.25.109},
file = {:Users/dorislee/Dropbox/Papers/Vittayakorn, Hays{\_}2011{\_}Quality Assessment for Crowdsourced Object Annotations.pdf:pdf},
isbn = {1-901725-43-X},
journal = {Procedings of the British Machine Vision Conference},
mendeley-groups = {seg},
pages = {109.1--109.11},
title = {{Quality Assessment for Crowdsourced Object Annotations}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper109/index.html},
year = {2011}
}
@article{Sameki2015,
author = {Sameki, Mehrnoosh and Gurari, Danna and Betke, Margrit},
file = {:Users/dorislee/Dropbox/Papers/Sameki, Gurari, Betke{\_}2015{\_}Characterizing Image Segmentation Behavior of the Crowd.pdf:pdf},
mendeley-groups = {seg},
pages = {1--4},
title = {{Characterizing Image Segmentation Behavior of the Crowd}},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:Users/dorislee/Dropbox/Papers/Krizhevsky, Sutskever, Hinton{\_}2012{\_}ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
mendeley-groups = {seg},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}

@article{OCWelinder2010,
abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
author = {Welinder, Peter and Perona, Pietro},
doi = {10.1109/CVPRW.2010.5543189},
file = {:Users/dorislee/Dropbox/Papers/Welinder, Perona{\_}2010{\_}Online crowdsourcing Rating annotators and obtaining cost-effective labels.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
mendeley-groups = {HCI/AI+HCI/crowd/crowd-seg},
pages = {25--32},
title = {{Online crowdsourcing: Rating annotators and obtaining cost-effective labels}},
year = {2010a}
}

@InProceedings{MartinFTM01,
  author = {D. Martin and C. Fowlkes and D. Tal and J. Malik},
  title = {A Database of Human Segmented Natural Images and its
           Application to Evaluating Segmentation Algorithms and
           Measuring Ecological Statistics},
  booktitle = {Proc. 8th Int'l Conf. Computer Vision},
  year = {2001},
  month = {July},
  volume = {2},
  pages = {416--423}
}
@Article{Everingham15, 
   author = "Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.", 
   title = "The Pascal Visual Object Classes Challenge: A Retrospective", 
   journal = "International Journal of Computer Vision", 
   volume = "111", 
   year = "2015", 
   number = "1", 
   month = jan, 
   pages = "98--136", 
} 

    
    
@inproceedings{zamir1998web,
  title={Web document clustering: A feasibility demonstration},
  author={Zamir, Oren and Etzioni, Oren},
  booktitle={Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={46--54},
  year={1998},
  organization={ACM}
}
@inproceedings{smith1997visualseek,
  title={VisualSEEk: a fully automated content-based image query system},
  author={Smith, John R and Chang, Shih-Fu},
  booktitle={Proceedings of the fourth ACM international conference on Multimedia},
  pages={87--98},
  year={1997},
  organization={ACM}
}
@article{chen2005clue,
  title={CLUE: cluster-based retrieval of images by unsupervised learning},
  author={Chen, Yixin and Wang, James Z and Krovetz, Robert},
  journal={Image Processing, IEEE Transactions on},
  volume={14},
  number={8},
  pages={1187--1201},
  year={2005},
  publisher={IEEE}
}
@inproceedings{ben2006improvingweb,
  title={Improvingweb-based image search via content based clustering},
  author={Ben-Haim, Nadav and Babenko, Boris and Belongie, Serge},
  booktitle={Computer Vision and Pattern Recognition Workshop, 2006. CVPRW'06. Conference on},
  pages={106--106},
  year={2006},
  organization={IEEE}
}
@inproceedings{sivic2005discovering,
  title={Discovering objects and their location in images},
  author={Sivic, Josef and Russell, Bryan C and Efros, Alexei and Zisserman, Andrew and Freeman, William T},
  booktitle={Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on},
  volume={1},
  pages={370--377},
  year={2005},
  organization={IEEE}
}
@inproceedings{felzenszwalb2008discriminatively,
  title={A discriminatively trained, multiscale, deformable part model},
  author={Felzenszwalb, Pedro and McAllester, David and Ramanan, Deva},
  booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  pages={1--8},
  year={2008},
  organization={IEEE}
}
@article{viola2004robust,
  title={Robust real-time face detection},
  author={Viola, Paul and Jones, Michael J},
  journal={International journal of computer vision},
  volume={57},
  number={2},
  pages={137--154},
  year={2004},
  publisher={Springer}
}
@inproceedings{torralba2004sharing,
  title={Sharing features: efficient boosting procedures for multiclass object detection},
  author={Torralba, Antonio and Murphy, Kevin P and Freeman, William T},
  booktitle={Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on},
  volume={2},
  pages={II--762},
  year={2004},
  organization={IEEE}
}
@article{torralba2003contextual,
  title={Contextual priming for object detection},
  author={Torralba, Antonio},
  journal={International journal of computer vision},
  volume={53},
  number={2},
  pages={169--191},
  year={2003},
  publisher={Springer}
}
@inproceedings{fe2003bayesian,
  title={A Bayesian approach to unsupervised one-shot learning of object categories},
  author={Fe-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on},
  pages={1134--1141},
  year={2003},
  organization={IEEE}
}
@article{Song2018,
author = {Song, Jean Y. and Fok, Raymond and Lundgard, Alan and Yang, Fang and Kim, Juho and Lasecki, Walter S.},
file = {:Users/dorislee/Box/Papers/FourEyes{\_}IUI2018.pdf:pdf},
isbn = {9781450349451},
journal = {Proceedings of the International Conference on Intelligent User Interfaces},
title = {{Two Tools are Better Than One : Tool Diversity as a Means of Improving Aggregate Crowd Performance}},
year = {2018}
}
@article{Gurari2015,
abstract = {Analyses of biomedical images often rely on demarcat-ing the boundaries of biological structures (segmentation). While numerous approaches are adopted to address the segmentation problem including collecting annotations from domain-experts and automated algorithms, the lack of comparative benchmarking makes it challenging to determine the current state-of-art, recognize limitations of existing approaches, and identify relevant future research directions. To provide practical guidance, we evalu-ated and compared the performance of trained experts, crowdsourced non-experts, and algorithms for annotating 305 objects coming from six datasets that include phase contrast, fluorescence, and magnetic resonance images. Compared to the gold standard established by expert consensus, we found the best annotators were experts, fol-lowed by non-experts, and then algorithms. This analysis revealed that online paid crowdsourced workers without domain-specific backgrounds are reliable annotators to use as part of the laboratory protocol for segmenting biomedical images. We also found that fusing the seg-mentations created by crowdsourced internet workers and algorithms yielded improved segmentation results over segmentations created by single crowdsourced or algorithm annotations respectively. We invite extensions of our work by sharing our data sets and associated segmentation annotations (http://www.cs.bu.edu/???betke/ BiomedicalImageSegmentation).},
author = {Gurari, Danna and Theriault, Diane and Sameki, Mehrnoosh and Isenberg, Brett and Pham, Tuan A. and Purwada, Alberto and Solski, Patricia and Walker, Matthew and Zhang, Chentian and Wong, Joyce Y. and Betke, Margrit},
doi = {10.1109/WACV.2015.160},
file = {:Users/dorislee/Box/Papers/07046014.pdf:pdf},
isbn = {9781479966820},
journal = {Proceedings of 2015 IEEE Winter Conference on Applications of Computer Vision (WACV) 2015},
pages = {1169--1176},
title = {{How to collect segmentations for biomedical images? A benchmark evaluating the performance of experts, crowdsourced non-experts, and algorithms}},
year = {2015}
}
@article{Gurari2016,
author = {Gurari, Danna and Sameki, Mehrnoosh and Wu, Zheng and Betke, Margrit},
file = {:Users/dorislee/Box/Papers/MiccaiImic2016{\_}SAVE{\_}System.pdf:pdf},
journal = {Medical Image Computing and Computer Assisted Intervention Interactive Medical Image Computation Workshop},
pages = {1--8},
title = {{Mixing Crowd and Algorithm Efforts to Segment Objects in Biomedical Images}},
year = {2016}
}

@article{Sorokin2008,
abstract = {We show how to outsource data annotation to Amazon Mechanical Turk. Doing so has produced annotations in quite large numbers relatively cheaply. The quality is good, and can be checked and controlled. Annotations are pro-duced quickly. We describe results for several different an-notation problems. We describe some strategies for deter-mining when the task is well specified and properly priced.},
author = {Sorokin, Alexander and Forsyth, David},
doi = {10.1109/CVPRW.2008.4562953},
file = {:Users/dorislee/Library/Application Support/Mendeley Desktop/Downloaded/Sorokin, Forsyth - 2008 - Utility data annotaton with Amazon Mechanical Turk.pdf:pdf},
isbn = {9781424423408},
issn = {2160-7508},
journal = {Proceedings of the 1st IEEE Workshop on Internet Vision at CVPR 08},
mendeley-groups = {crowd-seg},
number = {c},
pages = {1--8},
title = {{Utility data annotaton with Amazon Mechanical Turk}},
year = {2008}
}

@article{Gurari2018,
abstract = {We propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems. Specifically, we distinguish between images which lead multiple annotators to segment different foreground objects (ambiguous) versus minor inter-annotator differences of the same object. Taking images from eight widely used datasets, we crowdsource labeling the images as "ambiguous" or "not ambiguous" to segment in order to construct a new dataset we call STATIC. Using STATIC, we develop a system that automatically predicts which images are ambiguous. Experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment. Finally, we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid "ground truth" foreground object segmentations by collecting extra segmentations only when ambiguity is expected. Experiments show our system eliminates up to 47{\%} of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths.},
archivePrefix = {arXiv},
arxivId = {1705.00366},
author = {Gurari, Danna and He, Kun and Xiong, Bo and Zhang, Jianming and Sameki, Mehrnoosh and Jain, Suyog Dutt and Sclaroff, Stan and Betke, Margrit and Grauman, Kristen},
doi = {10.1007/s11263-018-1065-7},
eprint = {1705.00366},
file = {:Users/dorislee/Box/Papers/1705.00366.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {crowdsourcing,salient object detection,segmentation},
title = {{Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s)}},
url = {http://arxiv.org/abs/1705.00366},
year = {2018}
}

@INPROCEEDINGS{Natonek1998, 
author={E. Natonek}, 
booktitle={Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)}, 
title={Fast range image segmentation for servicing robots}, 
year={1998}, 
volume={1}, 
number={}, 
pages={406-411 vol.1}, 
keywords={CCD image sensors;distance measurement;image segmentation;mobile robots;object detection;object recognition;path planning;robot vision;3D scan line portioning;fast range image segmentation;grasping;object recognition;obstacle detection;planar patches;planar surface;real time range sensor;robot navigation;servicing robots;straight-line segment;Cameras;Grippers;Humans;Image segmentation;Navigation;Object recognition;Orbital robotics;Partitioning algorithms;Robot sensing systems;Robot vision systems}, 
doi={10.1109/ROBOT.1998.676445}, 
ISSN={1050-4729}, 
month={May},}
